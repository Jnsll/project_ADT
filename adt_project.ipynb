{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "#coding: utf-8\n",
    "\n",
    "#import re, glob\n",
    "#dirname='/media/DATA/Master/Analyse_donnees_text/data_suj1/train/'\n",
    "#d_entities={}\n",
    "#files=glob.glob(dirname+'*.ent')\n",
    "#for file in files:\n",
    "#    with open(file, 'r') as f_in :\n",
    "#        for line in f_in :\n",
    "#            line = re.sub(r'\\n', '', line)\n",
    "#            numId = re.sub(r\"(.+)\\t([A-Z][a-z]+)\\t(.+)\", r'\\1', line)\n",
    "#            cat = re.sub(r\"(.+)\\t([A-Z][a-z]+)\\t(.+)\", r'\\2', line)\n",
    "#            nom = re.sub(r\"(.+)\\t([A-Z][a-z]+)\\t([0-9]+-[0-9]+) ([A-Za-z]+)\", r'\\4', line)\n",
    "#            if re.match(r\"[0-9]+\", numId) and nom not in d_entities:\n",
    "#                d_entities[nom]=cat\n",
    "#print(d_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, glob, nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "dirname='/media/DATA/Master/Analyse_donnees_text/data_suj1/'\n",
    "d_entities={}\n",
    "p=re.compile(r'\\d+\\t([A-Za-z]+)\\s\\d+-\\d+\\s([A-Za-z ]+)(\\s\\d+-\\d+\\s([A-Za-z ]+))?')\n",
    "\n",
    "files=glob.glob(dirname+'train/*.ent')\n",
    "for file in files:\n",
    "    with open(file, 'r') as fichier:\n",
    "        for line in fichier:\n",
    "            m=p.match(line)\n",
    "            if not m: break\n",
    "            cat = m.group(1)\n",
    "            nom = m.group(2)\n",
    "            if cat =='Bacteria': continue\n",
    "            #if nom not in d_entities:\n",
    "            mots=word_tokenize(nom, 'english')\n",
    "            #print(len(tuple(mots)))\n",
    "            d_entities[tuple(mots)]=cat\n",
    "            if m.group(4) is not None: # En cas de deux entites sur la meme ligne \n",
    "                d_entities[tuple(word_tokenize(m.group(4), 'english'))]=cat\n",
    "                \n",
    "#print(d_entities)\n",
    "annotation=[]\n",
    "file='/media/DATA/Master/Analyse_donnees_text/data_suj1/dev/BTID-10095.txt'\n",
    "with open(file, 'r') as fichier:\n",
    "    text=fichier.read()\n",
    "    words=word_tokenize(text, 'english')\n",
    "#print(words)\n",
    "position=0\n",
    "for z in range(len(words)):\n",
    "    #print('z' + str(z))\n",
    "    position+=len(words[z])+1\n",
    "    for entity in d_entities:\n",
    "        #print(entity)\n",
    "        if len(words)-z-1<len(entity)-1: continue\n",
    "        if words[z]!= entity[0]: continue\n",
    "        detect=True\n",
    "        for i in range(1,len(entity)):\n",
    "            #print('i :' + str(i))\n",
    "            #print(entity[i])\n",
    "            if words[z+i] != entity[i]: \n",
    "                detect=False\n",
    "                break\n",
    "        if detect:\n",
    "            deb=position-len(words[z])\n",
    "            if len(entity)!=1:\n",
    "                for w in range(1,len(entity)):\n",
    "                    print(position)\n",
    "                    position+=len(entity[w])\n",
    "            annotation.append([d_entities[entity], ' '.join(entity), str(deb) +'-'+str(position)])\n",
    "            z=z+len(entity)\n",
    "            #print('z new:' + str(z))\n",
    "    \n",
    "#print(annotation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk,glob\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "environb=['environment', 'environments', 'medium', 'location', 'sample', 'nature', 'extract']\n",
    "environa=['tropical', 'subtropical', 'permafrost']\n",
    "noenv=['different', 'harsh','mild','normal','ordinary','safe','target','unique','various']\n",
    "water=['lake', 'lakes', 'waters', 'water', 'spring', 'marine', 'aquatic', 'sea', 'ocean', 'rain', 'meromictic', 'river','aquarium', 'wastewater']\n",
    "host=['murine', 'bovine', 'tick', 'microbe','microorganism','non-human','pest']\n",
    "hostpart=['cell','cell-wall','filaments','flagellum','macrophage','organelle','peptoglycan layer','rhizosphere','spore','tissue','abcesses','excretions','fluids','lesions','phyllome','rhizome','secretions','tumors','wounds','intestinal', 'blood', 'intracellular', 'root', 'heart', 'endophyte', 'endosymbiont', 'epiphytic']\n",
    "geo=['hospital', 'Institute']\n",
    "food=['food', 'meat', 'vegetables']\n",
    "soil=['soil', 'soils']\n",
    "medical= ['catheter', 'nosocomial', 'therapeutic', 'pharmaceutic', 'vaccine', 'gloves', 'scissors', 'scalpel']\n",
    "\n",
    "dirname='/media/DATA/Master/Analyse_donnees_text/data_suj1/dev/'\n",
    "files=glob.glob(dirname+'*.txt')\n",
    "\n",
    "#for file in files:\n",
    "file='/media/DATA/Master/Analyse_donnees_text/data_suj1/dev/BTID-20046.txt'\n",
    "with open(file, 'r') as fichier:\n",
    "    text=fichier.read()\n",
    "    words=word_tokenize(text, 'english')\n",
    "tagged=nltk.pos_tag(words)\n",
    "#print(tagged)\n",
    "compteur = 0\n",
    "annot={}\n",
    "for mot in tagged:\n",
    "    if mot[0] == 'in': # Entities detected by introduction with 'in' \n",
    "        if tagged[compteur+1][1]== 'NN' and tagged[compteur+2][1]== 'NN':\n",
    "            annot[tagged[compteur+1][0] + ' ' + tagged[compteur+2][0]]='Environment'\n",
    "        if tagged[compteur+1][1]== 'DT' and tagged[compteur+2][1]== 'NN' and tagged[compteur+3][1]== 'NN':\n",
    "            annot[tagged[compteur+2][0] + ' ' + tagged[compteur+3][0]]='Environment'\n",
    "        if tagged[compteur+1][1]=='DT' and tagged[compteur+2][1]=='JJ' and tagged[compteur+3][1]=='NN':\n",
    "            annot[tagged[compteur+2][0] + ' ' + tagged[compteur+3][0]]='Environment'    \n",
    "        if tagged[compteur+1][1]=='NNP' and tagged[compteur+2][1] != 'NN': #Geographical\n",
    "            annot[tagged[compteur+1][0]]='Geographical'\n",
    "    if mot[0] == 'within': # Entities detected by introduction with 'within' \n",
    "        if tagged[compteur+1][1]== 'DT' and tagged[compteur+2][1]== 'NN' and tagged[compteur+3][1]== 'NN'and tagged[compteur+4][1]== 'NN':\n",
    "            annot[tagged[compteur+2][0] + ' ' + tagged[compteur+3][0] + ' ' + tagged[compteur+4][0]]='HostPart'\n",
    "    # Entities detected with using a 'database' of target words\n",
    "    if mot[0] in environb: \n",
    "        if tagged[compteur-1][1]=='JJ' and tagged[compteur-1][0] not in noenv:\n",
    "            annot[tagged[compteur-1][0]+ ' ' + mot[0]]='Environment'\n",
    "    if mot[0] in environa:\n",
    "        if tagged[compteur+1][1]=='NN':\n",
    "            annot[mot[0]+ ' ' +tagged[compteur+1][0]]='Environment'\n",
    "    if mot[0] in water:\n",
    "        annot[tagged[compteur][0]]='Water'\n",
    "    if mot[0] in medical:\n",
    "        annot[tagged[compteur][0]]='Medical'\n",
    "    if mot[0] in soil:\n",
    "        annot[tagged[compteur][0]]='Soil'\n",
    "    if mot[0] in food:\n",
    "        annot[tagged[compteur][0]]='Food'\n",
    "    if mot[0] in host:\n",
    "        annot[tagged[compteur][0]]='Host'\n",
    "    if mot[0] in hostpart:\n",
    "        annot[tagged[compteur][0]]='HostPart'\n",
    "    if mot[0] in geo:\n",
    "        annot[tagged[compteur][0]]='Geographical'\n",
    "\n",
    "    compteur += 1 #Following word in text\n",
    "#print(annot)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-1-6e477c26ef27>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-6e477c26ef27>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "#print(d_entities)\n",
    "\n",
    "file='/media/DATA/Master/Analyse_donnees_text/data_suj1/dev/BTID-10095.txt'\n",
    "with open(file, 'r') as fichier:\n",
    "    with open('/media/DATA/Master/Analyse_donnees_text/test1', 'w') as fichier2:\n",
    "        for line in file:\n",
    "            text=fichier.read()\n",
    "            words=word_tokenize(text, 'english')\n",
    "            for word in words:\n",
    "                if word in d_entities:\n",
    "                    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting localisations by detecting names of countries in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries = {}\n",
    "countries_file = '/media/DATA/Master/Analyse_donnees_text/project_ADT/country_names'\n",
    "with open(countries_file, 'r') as c_file:\n",
    "    for line in c_file:\n",
    "        country = line.rstrip('\\n')\n",
    "        taille = len(country.split())\n",
    "        if taille in countries:\n",
    "            countries[taille].append(tuple(country.split()))\n",
    "        else:\n",
    "            countries[taille] = [tuple(country.split())]\n",
    "\n",
    "#print(countries)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Finland', '363-370']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "localisations = []\n",
    "file='/media/DATA/Master/Analyse_donnees_text/data_suj1/dev/BTID-20046.txt'\n",
    "with open(file, 'r') as fichier:\n",
    "    text=fichier.read()\n",
    "    words=word_tokenize(text, 'english')\n",
    "position=0\n",
    "for z in range(len(words)):\n",
    "    position += len(words[z]) + 1\n",
    "    for taille in countries:\n",
    "        if len(words)-z-1<taille-1: break\n",
    "        for pays in countries[taille]:\n",
    "            if words[z] != pays[0]: continue\n",
    "            detect=True\n",
    "            for i in range(1,len(pays)):\n",
    "                if words[z+i] != pays[i]: \n",
    "                    detect=False\n",
    "                    break\n",
    "            if detect:\n",
    "                deb=position-len(words[z])\n",
    "                if len(pays)!=1:\n",
    "                    for w in range(1,len(entity)):\n",
    "                        position+=len(entity[w])\n",
    "                localisations.append([' '.join(pays), str(deb) +'-'+str(position)])\n",
    "                z=z+taille\n",
    "print(localisations)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
