{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "#coding: utf-8\n",
    "\n",
    "#import re, glob\n",
    "#dirname='/media/DATA/Master/Analyse_donnees_text/data_suj1/train/'\n",
    "#d_entities={}\n",
    "#files=glob.glob(dirname+'*.ent')\n",
    "#for file in files:\n",
    "#    with open(file, 'r') as f_in :\n",
    "#        for line in f_in :\n",
    "#            line = re.sub(r'\\n', '', line)\n",
    "#            numId = re.sub(r\"(.+)\\t([A-Z][a-z]+)\\t(.+)\", r'\\1', line)\n",
    "#            cat = re.sub(r\"(.+)\\t([A-Z][a-z]+)\\t(.+)\", r'\\2', line)\n",
    "#            nom = re.sub(r\"(.+)\\t([A-Z][a-z]+)\\t([0-9]+-[0-9]+) ([A-Za-z]+)\", r'\\4', line)\n",
    "#            if re.match(r\"[0-9]+\", numId) and nom not in d_entities:\n",
    "#                d_entities[nom]=cat\n",
    "#print(d_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Host', 'plant', '156-161'], ['HostPart', 'root', '290-294'], ['Host', 'plant', '400-405'], ['HostPart', 'cell', '401-405'], ['HostPart', 'cell', '406-410'], ['Soil', 'nitrogen', '416-424'], ['HostPart', 'root', '607-611'], ['HostPart', 'chromosome', '642-652'], ['Host', 'plant', '770-775'], ['Host', 'plant', '807-812'], ['HostPart', 'root', '848-852'], ['HostPart', 'cell', '928-932'], ['HostPart', 'membrane', '961-969'], ['Host', 'plant', '1125-1130'], ['Geographical', 'Florida', '1244-1251'], ['Geographical', 'USA', '1254-1257'], ['Soil', 'nitrogen', '1322-1330'], ['Soil', 'nitrogen', '1456-1464'], ['HostPart', 'root', '1478-1482'], ['HostPart', 'chromosome', '1506-1516'], ['Geographical', 'Japan', '1979-1984']]\n"
     ]
    }
   ],
   "source": [
    "import re, glob, nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "dirname='/media/DATA/Master/Analyse_donnees_text/data_suj1/'\n",
    "d_entities={}\n",
    "p=re.compile(r'\\d+\\t([A-Za-z]+)\\s\\d+-\\d+\\s([A-Za-z ]+)(\\s\\d+-\\d+\\s([A-Za-z ]+))?')\n",
    "\n",
    "files=glob.glob(dirname+'train/*.ent')\n",
    "for file in files:\n",
    "    with open(file, 'r') as fichier:\n",
    "        for line in fichier:\n",
    "            m=p.match(line)\n",
    "            if not m: break\n",
    "            cat = m.group(1)\n",
    "            nom = m.group(2)\n",
    "            if cat =='Bacteria': continue\n",
    "            #if nom not in d_entities:\n",
    "            mots=word_tokenize(nom, 'english')\n",
    "            #print(len(tuple(mots)))\n",
    "            d_entities[tuple(mots)]=cat\n",
    "            if m.group(4) is not None: # En cas de deux entites sur la meme ligne \n",
    "                d_entities[tuple(word_tokenize(m.group(4), 'english'))]=cat\n",
    "                \n",
    "#print(d_entities)\n",
    "annotation=[]\n",
    "file='/media/DATA/Master/Analyse_donnees_text/data_suj1/dev/BTID-10095.txt'\n",
    "with open(file, 'r') as fichier:\n",
    "    text=fichier.read()\n",
    "    words=word_tokenize(text, 'english')\n",
    "#print(words)\n",
    "position=0\n",
    "for z in range(len(words)):\n",
    "    #print('z' + str(z))\n",
    "    position+=len(words[z])+1\n",
    "    for entity in d_entities:\n",
    "        #print(entity)\n",
    "        if len(words)-z-1<len(entity)-1: continue\n",
    "        if words[z]!= entity[0]: continue\n",
    "        detect=True\n",
    "        for i in range(1,len(entity)):\n",
    "            #print('i :' + str(i))\n",
    "            #print(entity[i])\n",
    "            if words[z+i] != entity[i]: \n",
    "                detect=False\n",
    "                break\n",
    "        if detect:\n",
    "            deb=position-len(words[z])\n",
    "            if len(entity)!=1:\n",
    "                for w in range(1,len(entity)):\n",
    "                    print(position)\n",
    "                    position+=len(entity[w])\n",
    "            annotation.append([d_entities[entity], ' '.join(entity), str(deb) +'-'+str(position)])\n",
    "            z=z+len(entity)\n",
    "            #print('z new:' + str(z))\n",
    "    \n",
    "print(annotation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in root nodulation\n",
      "in growth mode\n",
      "in freshwater medium\n"
     ]
    }
   ],
   "source": [
    "import nltk,glob\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "dirname='/media/DATA/Master/Analyse_donnees_text/data_suj1/dev/'\n",
    "files=glob.glob(dirname+'*.txt')\n",
    "\n",
    "for file in files:\n",
    "#file='/media/DATA/Master/Analyse_donnees_text/data_suj1/dev/BTID-10095.txt'\n",
    "    with open(file, 'r') as fichier:\n",
    "        text=fichier.read()\n",
    "        words=word_tokenize(text, 'english')\n",
    "        p#rint(words)\n",
    "        tagged=nltk.pos_tag(words)\n",
    "        #print(tagged)\n",
    "        dico = {}\n",
    "        compteur = 0\n",
    "        for mot in tagged:\n",
    "            #print(mot[0])\n",
    "            if mot[0] == 'in':\n",
    "                if tagged[compteur+1][1]== 'NN' and tagged[compteur+2][1]== 'NN':\n",
    "                    print(mot[0] + ' ' +tagged[compteur+1][0] + ' ' + tagged[compteur+2][0])\n",
    "            compteur += 1\n",
    "       \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-1-6e477c26ef27>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-6e477c26ef27>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "#print(d_entities)\n",
    "\n",
    "file='/media/DATA/Master/Analyse_donnees_text/data_suj1/dev/BTID-10095.txt'\n",
    "with open(file, 'r') as fichier:\n",
    "    with open('/media/DATA/Master/Analyse_donnees_text/test1', 'w') as fichier2:\n",
    "        for line in file:\n",
    "            text=fichier.read()\n",
    "            words=word_tokenize(text, 'english')\n",
    "            for word in words:\n",
    "                if word in d_entities:\n",
    "                    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting localisations by detecting names of countries in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: ['Andorra', 'Afghanistan', 'Anguilla', 'Albania', 'Armenia', 'Angola', 'Antarctica', 'Argentina', 'Austria', 'Australia', 'Aruba', 'Azerbaijan', 'Barbados', 'Bangladesh', 'Belgium', 'Bulgaria', 'Bahrain', 'Burundi', 'Benin', 'Bermuda', 'Brunei', 'Bolivia', 'Brazil', 'Bahamas', 'Bhutan', 'Botswana', 'Belarus', 'Belize', 'Canada', 'Switzerland', 'Chile', 'Cameroon', 'China', 'Colombia', 'Cuba', 'Curacao', 'Cyprus', 'Czechia', 'Germany', 'Djibouti', 'Denmark', 'Dominica', 'Algeria', 'Ecuador', 'Estonia', 'Egypt', 'Eritrea', 'Spain', 'Ethiopia', 'Finland', 'Fiji', 'Micronesia', 'France', 'Gabon', 'Grenada', 'Georgia', 'Guernsey', 'Ghana', 'Gibraltar', 'Greenland', 'Gambia', 'Guinea', 'Guadeloupe', 'Greece', 'Guatemala', 'Guam', 'Guinea-Bissau', 'Guyana', 'Honduras', 'Croatia', 'Haiti', 'Hungary', 'Indonesia', 'Ireland', 'Israel', 'India', 'Iraq', 'Iran', 'Iceland', 'Italy', 'Jersey', 'Jamaica', 'Jordan', 'Japan', 'Kenya', 'Kyrgyzstan', 'Cambodia', 'Kiribati', 'Comoros', 'Kosovo', 'Kuwait', 'Kazakhstan', 'Laos', 'Lebanon', 'Liechtenstein', 'Liberia', 'Lesotho', 'Lithuania', 'Luxembourg', 'Latvia', 'Libya', 'Morocco', 'Monaco', 'Moldova', 'Montenegro', 'Madagascar', 'Macedonia', 'Mali', 'Myanmar', 'Mongolia', 'Macao', 'Martinique', 'Mauritania', 'Montserrat', 'Malta', 'Mauritius', 'Maldives', 'Malawi', 'Mexico', 'Malaysia', 'Mozambique', 'Namibia', 'Niger', 'Nigeria', 'Nicaragua', 'Netherlands', 'Norway', 'Nepal', 'Nauru', 'Niue', 'Oman', 'Panama', 'Peru', 'Philippines', 'Pakistan', 'Poland', 'Pitcairn', 'Portugal', 'Palau', 'Paraguay', 'Qatar', 'Reunion', 'Romania', 'Serbia', 'Russia', 'Rwanda', 'Seychelles', 'Sudan', 'Sweden', 'Singapore', 'Slovenia', 'Slovakia', 'Senegal', 'Somalia', 'Suriname', 'Syria', 'Swaziland', 'Chad', 'Togo', 'Thailand', 'Tajikistan', 'Tokelau', 'Turkmenistan', 'Tunisia', 'Tonga', 'Turkey', 'Tuvalu', 'Taiwan', 'Tanzania', 'Ukraine', 'Uganda', 'Uruguay', 'Uzbekistan', 'Vatican', 'Venezuela', 'Vietnam', 'Vanuatu', 'Samoa', 'Yemen', 'Mayotte', 'Zambia', 'Zimbabwe'], 2: ['American Samoa', 'Aland Islands', 'Burkina Faso', 'Saint Barthelemy', 'Bouvet Island', 'Cocos Islands', 'Ivory Coast', 'Cook Islands', 'Costa Rica', 'Cape Verde', 'Christmas Island', 'Dominican Republic', 'Western Sahara', 'Falkland Islands', 'Faroe Islands', 'United Kingdom', 'French Guiana', 'Equatorial Guinea', 'Hong Kong', 'North Korea', 'South Korea', 'Cayman Islands', 'Saint Lucia', 'Sri Lanka', 'Saint Martin', 'Marshall Islands', 'New Caledonia', 'Norfolk Island', 'New Zealand', 'French Polynesia', 'Puerto Rico', 'Palestinian Territory', 'Saudi Arabia', 'Solomon Islands', 'South Sudan', 'Saint Helena', 'Sierra Leone', 'San Marino', 'El Salvador', 'Sint Maarten', 'East Timor', 'United States', 'South Africa', 'Netherlands Antilles'], 3: ['United Arab Emirates', 'Antigua and Barbuda', 'Bosnia and Herzegovina', 'Central African Republic', 'Isle of Man', 'Northern Mariana Islands', 'Papua New Guinea', 'French Southern Territories', 'Trinidad and Tobago', 'British Virgin Islands', 'U.S. Virgin Islands', 'Wallis and Futuna', 'Serbia and Montenegro'], 4: ['Republic of the Congo', 'British Indian Ocean Territory', 'Saint Kitts and Nevis', 'Saint Pierre and Miquelon', 'Svalbard and Jan Mayen', 'Sao Tome and Principe', 'Turks and Caicos Islands'], 5: ['Bonaire, Saint Eustatius and Saba ', 'Democratic Republic of the Congo', 'Heard Island and McDonald Islands', 'United States Minor Outlying Islands', 'Saint Vincent and the Grenadines'], 7: ['South Georgia and the South Sandwich Islands']}\n"
     ]
    }
   ],
   "source": [
    "countries = {}\n",
    "countries_file = '/media/DATA/Master/Analyse_donnees_text/project_ADT/country_names'\n",
    "with open(countries_file, 'r') as c_file:\n",
    "    for line in c_file:\n",
    "        country = line.rstrip('\\n')\n",
    "        taille = len(country.split())\n",
    "        if taille in countries:\n",
    "            countries[taille].append(country)\n",
    "        else:\n",
    "            countries[taille] = [country]\n",
    "\n",
    "#print(countries)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Japan']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "localisations = []\n",
    "file='/media/DATA/Master/Analyse_donnees_text/data_suj1/dev/BTID-10095.txt'\n",
    "with open(file, 'r') as fichier:\n",
    "    with open('/media/DATA/Master/Analyse_donnees_text/test1', 'w') as fichier2:\n",
    "        for line in file:\n",
    "            text=fichier.read()\n",
    "            words=word_tokenize(text, 'english')\n",
    "            for word in words:\n",
    "                if word in countries[1]:\n",
    "                    localisations.append(word)\n",
    "print(localisations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
